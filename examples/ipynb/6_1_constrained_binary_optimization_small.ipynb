{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small-scale Constained Binary Optimization with Choco-Q \n",
    "\n",
    "**Author:** Debin Xiang & Qifan Jiang\n",
    "\n",
    "**Date:** 02/02/2025\n",
    "\n",
    "Based on paper \"[Choco-Q: Commute Hamiltonian-based QAOA for Constrained Binary Optimization][1]\" (Accepted by HPCA 2025)\n",
    "\n",
    "[1]: https://ieeexplore.ieee.org/document/TBD\n",
    "\n",
    "Corresponding to Table 1 in the original paper, but with the Choco-Q* part omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import signal\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor, TimeoutError\n",
    "\n",
    "from janusq.application.chocoq.chocoq.problems.facility_location_problem import generate_flp\n",
    "from janusq.application.chocoq.chocoq.problems.graph_coloring_problem import generate_gcp\n",
    "from janusq.application.chocoq.chocoq.problems.k_partition_problem import generate_kpp\n",
    "from janusq.application.chocoq.chocoq.problems.job_scheduling_problem import generate_jsp\n",
    "from janusq.application.chocoq.chocoq.problems.traveling_salesman_problem import generate_tsp\n",
    "from janusq.application.chocoq.chocoq.problems.set_cover_problem import generate_scp\n",
    "from janusq.application.chocoq.chocoq.solvers.optimizers import CobylaOptimizer, AdamOptimizer\n",
    "from janusq.application.chocoq.chocoq.solvers.qiskit import (\n",
    "    PenaltySolver, CyclicSolver, HeaSolver, ChocoSolver, \n",
    "    AerGpuProvider, AerProvider, FakeBrisbaneProvider, FakeKyivProvider, FakeTorinoProvider, DdsimProvider,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)  # display all rows\n",
    "pd.set_option('display.max_columns', None)  # display all columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/chocoq_examples/small_scale\"\n",
    "num_cases = 10  # The number of cases in each benchmark\n",
    "problem_scale = 1 # The problem scale, 1 is the minimal scale like F1,K1,G1 in Table 1 of paper\n",
    "#2 means F2 K2 ... In CPU version, this benchmarks with higher scale is much slower when solving with baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the problem to be solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flp_problems_pkg, flp_configs_pkg = generate_flp(num_cases, [(1, 2), (2, 3), (3, 3), (3, 4)][:problem_scale], 1, 20)\n",
    "gcp_problems_pkg, gcp_configs_pkg = generate_gcp(num_cases, [(3, 1), (3, 2), (4, 2), (4, 3)][:problem_scale])\n",
    "kpp_problems_pkg, kpp_configs_pkg = generate_kpp(num_cases, [(4, 2, 3), (6, 3, 5), (8, 3, 7), (9, 3, 8)][:problem_scale], 1, 20)\n",
    "\n",
    "configs_pkg = flp_configs_pkg + gcp_configs_pkg + kpp_configs_pkg\n",
    "with open(f\"{file_path}/problem.config\", \"w\") as file:\n",
    "    for pkid, configs in enumerate(configs_pkg):\n",
    "        for problem in configs:\n",
    "            file.write(f'{pkid}: {problem}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on circuit depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_pkg = flp_problems_pkg + gcp_problems_pkg + kpp_problems_pkg\n",
    "\n",
    "metrics_lst = ['culled_depth', 'num_params']\n",
    "solvers = [PenaltySolver, CyclicSolver, HeaSolver, ChocoSolver]\n",
    "headers = [\"pkid\", 'method', 'layers'] + metrics_lst\n",
    "\n",
    "def process_layer(prb, num_layers, solver, metrics_lst):\n",
    "    opt = CobylaOptimizer(max_iter=200)\n",
    "    ddsim = DdsimProvider()\n",
    "    cpu = AerProvider()\n",
    "    gpu = AerGpuProvider()\n",
    "    used_solver = solver(\n",
    "        prb_model = prb,\n",
    "        optimizer = opt,\n",
    "        # Select CPU or GPU simulator\n",
    "        # cpu simulator, comment it when use GPU\n",
    "        provider = cpu if solver in [PenaltySolver, CyclicSolver, HeaSolver] else ddsim,\n",
    "        # uncomment the line below to use GPU simulator\n",
    "        # provider = gpu if solver in [PenaltySolver, CyclicSolver, HeaSolver] else ddsim,\n",
    "        num_layers = num_layers,\n",
    "        shots = 1024,\n",
    "    )\n",
    "    metrics = used_solver.circuit_analyze(metrics_lst)\n",
    "    return metrics\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_timeout = 60 * 60 * 24 # Set timeout duration\n",
    "    num_complete = 0\n",
    "    with open(f\"{file_path}/evaluate_depth.csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)  # Write headers once\n",
    "\n",
    "    num_processes_cpu = os.cpu_count() // 2\n",
    "    with ProcessPoolExecutor(max_workers=num_processes_cpu) as executor:\n",
    "        futures = []\n",
    "        for solver in solvers:\n",
    "            for pkid, problems in enumerate(problems_pkg):\n",
    "                for problem in problems:\n",
    "                    if solver == ChocoSolver:\n",
    "                        num_layers = 1\n",
    "                    else:\n",
    "                        num_layers = 5\n",
    "                    future = executor.submit(process_layer, problem, num_layers, solver, metrics_lst)\n",
    "                    futures.append((future, pkid, solver.__name__, num_layers))\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        for future, pkid, solver, num_layers in futures:\n",
    "            current_time = time.perf_counter()\n",
    "            remaining_time = max(set_timeout - (current_time - start_time), 0)\n",
    "            diff = []\n",
    "            try:\n",
    "                result = future.result(timeout=remaining_time)\n",
    "                diff.extend(result)\n",
    "                # print(f\"Task for problem {pkid}, num_layers {num_layers} executed successfully.\")\n",
    "            except MemoryError:\n",
    "                diff.append('memory_error')\n",
    "                print(f\"Task for problem {pkid}, num_layers {num_layers} encountered a MemoryError.\")\n",
    "            except TimeoutError:\n",
    "                diff.append('timeout')\n",
    "                print(f\"Task for problem {pkid}, num_layers {num_layers} timed out.\")\n",
    "            finally:\n",
    "                row = [pkid, solver, num_layers] + diff\n",
    "                with open(f\"{file_path}/evaluate_depth.csv\", mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(row)  # Write row immediately\n",
    "                num_complete += 1\n",
    "                if num_complete == len(futures):\n",
    "                    print(f'Data has been written to {file_path}/evaluate_depth.csv')\n",
    "                    for process in executor._processes.values():\n",
    "                        os.kill(process.pid, signal.SIGTERM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "\n",
    "grouped_df = df.groupby(['pkid', 'layers', 'method'], as_index=False).agg({\n",
    "    \"culled_depth\": 'mean',\n",
    "})\n",
    "\n",
    "values = [\"culled_depth\"]\n",
    "pivot_df = grouped_df.pivot(index =['pkid'], columns='method', values=values)\n",
    "\n",
    "method_order = ['PenaltySolver', 'CyclicSolver', 'HeaSolver', 'ChocoSolver']\n",
    "pivot_df = pivot_df.reindex(columns=pd.MultiIndex.from_product([values, method_order]))\n",
    "\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_pkg = list(\n",
    "    itertools.chain(\n",
    "        enumerate(flp_problems_pkg),\n",
    "        enumerate(gcp_problems_pkg),\n",
    "        enumerate(kpp_problems_pkg),\n",
    "    )\n",
    ")\n",
    "\n",
    "solvers = [PenaltySolver, CyclicSolver, HeaSolver, ChocoSolver]\n",
    "evaluation_metrics = ['best_solution_probs', 'in_constraints_probs', 'ARG', 'iteration_count', 'classcial', 'quantum', 'run_times']\n",
    "headers = ['pkid', 'pbid', 'layers', \"variables\", 'constraints', 'method'] + evaluation_metrics\n",
    "\n",
    "def process_layer(prb, num_layers, solver):\n",
    "    opt = CobylaOptimizer(max_iter=200)\n",
    "    ddsim = DdsimProvider()\n",
    "    cpu = AerProvider()\n",
    "    gpu = AerGpuProvider()\n",
    "    prb.set_penalty_lambda(400)\n",
    "    used_solver = solver(\n",
    "        prb_model = prb,\n",
    "        optimizer = opt,\n",
    "        provider = cpu if solver in [PenaltySolver, CyclicSolver, HeaSolver] else ddsim,\n",
    "        # provider = gpu if solver in [PenaltySolver, CyclicSolver, HeaSolver] else ddsim,\n",
    "        num_layers = num_layers,\n",
    "        shots = 1024,\n",
    "    )\n",
    "    used_solver.solve()\n",
    "    eval = used_solver.evaluation()\n",
    "    time = list(used_solver.time_analyze())\n",
    "    run_times = used_solver.run_counts()\n",
    "    return eval + time + [run_times]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_start_time = time.perf_counter()\n",
    "    set_timeout = 60 * 60 * 2 # Set timeout duration\n",
    "    num_complete = 0\n",
    "\n",
    "    with open(f\"{file_path}/evaluate_other.csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "\n",
    "    num_processes_cpu = os.cpu_count()\n",
    "    # pkid-pbid: problem package id and problem id\n",
    "    for pkid, (diff_level, problems) in enumerate(problems_pkg):\n",
    "        for solver in solvers:\n",
    "            if solver in [PenaltySolver, CyclicSolver, HeaSolver]:\n",
    "                num_processes = 2**(4 - diff_level) + 1\n",
    "            else:\n",
    "                num_processes = 100\n",
    "\n",
    "            with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "                futures = []\n",
    "                if solver == ChocoSolver:\n",
    "                    layer = 1\n",
    "                else:\n",
    "                    layer = 5\n",
    "\n",
    "                for pbid, prb in enumerate(problems):\n",
    "                    print(f'{pkid}-{pbid}, {layer}, {solver} build')\n",
    "                    future = executor.submit(process_layer, prb, layer, solver)\n",
    "                    futures.append((future, prb, pkid, pbid, layer, solver.__name__))\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "                for future, prb, pkid, pbid, layer, solver in futures:\n",
    "                    current_time = time.perf_counter()\n",
    "                    remaining_time = max(set_timeout - (current_time - start_time), 0)\n",
    "                    diff = []\n",
    "                    try:\n",
    "                        metrics = future.result(timeout=remaining_time)\n",
    "                        diff.extend(metrics)\n",
    "                        # print(f\"Task for problem {pkid}-{pbid} L={layer} {solver} executed successfully.\")\n",
    "                    except MemoryError:\n",
    "                        print(f\"Task for problem {pkid}-{pbid} L={layer} {solver} encountered a MemoryError.\")\n",
    "                        for dict_term in evaluation_metrics:\n",
    "                            diff.append('memory_error')\n",
    "                    except TimeoutError:\n",
    "                        print(f\"Task for problem {pkid}-{pbid} L={layer} {solver} timed out.\")\n",
    "                        for dict_term in evaluation_metrics:\n",
    "                            diff.append('timeout')\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "                    finally:\n",
    "                        row = [pkid, pbid, layer, len(prb.variables), len(prb.lin_constr_mtx), solver] + diff\n",
    "                        with open(f\"{file_path}/evaluate_other.csv\", mode='a', newline='') as file:\n",
    "                            writer = csv.writer(file)\n",
    "                            writer.writerow(row)  # Write row immediately\n",
    "                        num_complete += 1\n",
    "                        if num_complete == len(futures):\n",
    "                            print(f'problem_pkg_{pkid} has finished')\n",
    "                            for process in executor._processes.values():\n",
    "                                os.kill(process.pid, signal.SIGTERM)\n",
    "    print(f'Data has been written to {file_path}/evaluate_other.csv')\n",
    "    print(time.perf_counter()- all_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(f\"{file_path}/evaluate_depth.csv\")\n",
    "\n",
    "grouped_df1 = df1.groupby(['pkid', 'layers', 'method'], as_index=False).agg({\n",
    "    \"culled_depth\": 'mean',\n",
    "})\n",
    "\n",
    "pivot_df1 = grouped_df1.pivot(index=['pkid'], columns='method', values=[\"culled_depth\"])\n",
    "\n",
    "method_order1 = ['PenaltySolver', 'CyclicSolver', 'HeaSolver', 'ChocoSolver']\n",
    "pivot_df1 = pivot_df1.reindex(columns=pd.MultiIndex.from_product([[\"culled_depth\"], method_order1]))\n",
    "\n",
    "df2 = pd.read_csv(f\"{file_path}/evaluate_other.csv\")\n",
    "\n",
    "df2 = df2.drop(columns=['pbid'])\n",
    "df2 = df2[df2['ARG'] <= 100000]\n",
    "\n",
    "grouped_df2 = df2.groupby(['pkid', 'layers', 'variables', 'constraints', 'method'], as_index=False).agg({\n",
    "    \"ARG\": 'mean',\n",
    "    'in_constraints_probs': 'mean',\n",
    "    'best_solution_probs': 'mean',\n",
    "    'iteration_count': 'mean',\n",
    "    'classcial': 'mean',\n",
    "    'run_times': 'mean',\n",
    "})\n",
    "\n",
    "pivot_df2 = grouped_df2.pivot(index=['pkid', 'variables', 'constraints'], columns='method', values=[\"best_solution_probs\", 'in_constraints_probs', 'ARG'])\n",
    "\n",
    "method_order2 = ['PenaltySolver', 'CyclicSolver', 'HeaSolver', 'ChocoSolver']\n",
    "pivot_df2 = pivot_df2.reindex(columns=pd.MultiIndex.from_product([[\"best_solution_probs\", 'in_constraints_probs', 'ARG'], method_order2]))\n",
    "\n",
    "merged_df = pd.merge(pivot_df1, pivot_df2, on='pkid', how='inner')\n",
    "\n",
    "merged_df = merged_df[['culled_depth', 'best_solution_probs', 'in_constraints_probs']]\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'culled_depth': 'Circuit depth',\n",
    "    'best_solution_probs': 'Success rate (%)',\n",
    "    'in_constraints_probs': 'In-constraints rate (%)'\n",
    "})\n",
    "\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'PenaltySolver': 'Penalty',\n",
    "    'CyclicSolver': 'Cyclic',\n",
    "    'HeaSolver': 'HEA',\n",
    "    'ChocoSolver': 'Choco-Q'\n",
    "})\n",
    "\n",
    "merged_df.index = ['F1', 'G1', 'K1']\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The method we use to calculate the improvement is as follows: for each benchmark, we compute the improvement ratio. Then, we take the arithmetic mean of the improvement ratios across all benchmarks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_df' already contains the necessary data (after the previous steps)\n",
    "\n",
    "# Calculate the improvement for each row\n",
    "\n",
    "# Circuit depth improvement: cyclic / Choco-Q\n",
    "merged_df['Circuit_depth_improvement'] = merged_df[('Circuit depth', 'Cyclic')] / merged_df[('Circuit depth', 'Choco-Q')]\n",
    "\n",
    "# Success rate improvement: Choco-Q / cyclic\n",
    "merged_df['Success_rate_improvement'] = merged_df[('Success rate (%)', 'Choco-Q')] / merged_df[('Success rate (%)', 'Cyclic')]\n",
    "\n",
    "# In-constraints rate improvement: Choco-Q / cyclic\n",
    "merged_df['In_constraints_rate_improvement'] = merged_df[('In-constraints rate (%)', 'Choco-Q')] / merged_df[('In-constraints rate (%)', 'Cyclic')]\n",
    "\n",
    "# Filter out rows where any improvement column has a zero denominator or zero numerator (to avoid division by zero)\n",
    "valid_rows = merged_df[(merged_df[('Circuit depth', 'Cyclic')] != 0) & (merged_df[('Circuit depth', 'Choco-Q')] != 0) &\n",
    "                       (merged_df[('Success rate (%)', 'Cyclic')] != 0) & (merged_df[('Success rate (%)', 'Choco-Q')] != 0) &\n",
    "                       (merged_df[('In-constraints rate (%)', 'Cyclic')] != 0) & (merged_df[('In-constraints rate (%)', 'Choco-Q')] != 0)]\n",
    "\n",
    "# Calculate the average improvement for each metric\n",
    "avg_circuit_depth_improvement = valid_rows['Circuit_depth_improvement'].mean()\n",
    "avg_success_rate_improvement = valid_rows['Success_rate_improvement'].mean()\n",
    "avg_in_constraints_rate_improvement = valid_rows['In_constraints_rate_improvement'].mean()\n",
    "\n",
    "improvement_table = pd.DataFrame({\n",
    "    'Circuit Depth': [avg_circuit_depth_improvement],\n",
    "    'Success Rate': [avg_success_rate_improvement],\n",
    "    'In-constraints Rate': [avg_in_constraints_rate_improvement]\n",
    "}, index=['Improvement relative to Cyclic'])\n",
    "\n",
    "improvement_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since all the experiments are run at scale 1 (the simplest problem), the performance of other baselines is also quite good. As a result, the improvement ratios appear to be smaller compared to those in Table 1 of the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janusq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
